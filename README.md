Databricks-14-Days-AI-Challenge:

This repository documents my hands-on journey through the 14 Days AI Challenge, focused on building an enterprise-grade Databricks Lakehouse architecture for large-scale ecommerce data ingestion, transformation, and using AI capabilities of Databricks.

The project demonstrates: - Real-world data engineering workflows - Unity Catalog + Volumes based ingestion - Bronze â†’ Silver â†’ Gold architecture - Apache Spark (PySpark) fundamentals - Preparation for NLP, Sentiment Analysis & Power BI integration

ğŸ“Œ Project Objectives
Build a Lakehouse architecture using Databricks\
Ingest large ecommerce datasets (5GB+)\
Use Unity Catalog + Volumes for governed storage\
Practice Apache Spark transformations\
Prepare data for NLP, sentiment analysis & BI\
Maintain Git-based version control using Databricks Repos + GitHub

ğŸ—ï¸ Architecture Overview (Lakehouse Pattern)
Source (CSV / ZIP files)
        â†“
Bronze Layer (Raw data in Volumes)
        â†“
Silver Layer (Cleaned & validated data)
        â†“
Gold Layer (Aggregations, KPIs, Features)
        â†“
BI / NLP / ML Models
